While we had the possibility to test a lot of algorithms on AutoMPG~\ref{db:ds1}, it was very hard if not impossible to achieve good results. We think that this is because of the small size of the training and test data, the missing values further complicate the appliance of machine learning. Although we saw differences between the algorithms, we have to conclude that even the best of them did not give very good predictions.\par
For Year Prediction MSD~\ref{db:ds2} and Electric Power Consumption~\ref{db:ds3} different algorithm performed best. Although the number of samples was much higher in EPC the lack of attributes compared to YPMSD seemed to have a big impact on the performance of the algorithms. While $k$-nearest neighbors worked very well for EPC, not only was it surpassed in terms of quality by other algorithms in YPMSD also its runtime was considerably worse compared to the other algorithms. This seems rational since the number of samples does not have a big impact on runtime of $k$-nearest neighbors in the implementation used by \textit{scikit}.\par
Despite the differences between the two datasets all algorithms generated reasonably good models only differing slightly in quality. We could not use Support Vector Machines any of these two datasets because of its huge runtime which is dependent on the number of samples.\par

It was also very interesting to see that for Stochastic Gradient Descent different $\epsilon$ performed best for the different datasets. While small $\epsilon$ resulted in very bad models for YPMSD it was a good choice for EPC and also on AutoMPG Stochastic Gradient Descent worked better with small $\epsilon$. Another interesting observation was that the choice of $\alpha$ did not have impact on model quality and had only minor impact on runtimes.
