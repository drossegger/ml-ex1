While we had the possibility to test a lot of algorithms on AutoMPG~\ref{db:sec:ds1}, it was very hard if not impossible to achieve good results. We think that this is because of the small size of the training and test data, the missing values further complicate the appliance of machine learning. Although we saw differences between the algorithms, we have to conclude that even the best of them did not give very good predictions.\par
For Year Prediction MSD~\ref{db:sec:ds2} and Electric Power Consumption~\ref{db:sec:ds3} different algorithm performed best. Although the number of samples was much higher in Electric Power Consumption the lack of attributes compared to Year Prediction MSD seemed to have a big impact on the performance of the algorithms. While $k$-nearest neighbors worked very well for Electric Power Consumption, not only was it surpassed in terms of quality by other algorithms in Year Prediction MSD also its runtime was considerably worse compared to the other algorithms. This seems rational since the number of samples does not have a big impact on runtime of $k$-nearest neighbors in the implementation used by \textit{scikit}.\par
Despite the differences between the two datasets all algorithms generated reasonably good models only differing slightly in quality. We could not use Support Vector Machines any of these two datasets because of its huge runtime which is dependent on the number of samples.\par

It was also very interesting to see that for Stochastic Gradient Descent different $\epsilon$ performed best for the different datasets. While small $\epsilon$ resulted in very bad models for Year Prediction MSD it was a good choice for Electric Power Consumption and also on AutoMPG Stochastic Gradient Descent worked better with small $\epsilon$. Another interesting observation was that the choice of $\alpha$ did not have impact on model quality and had only minor impact on runtimes.\par

For analysing the results we thought that for the two big datasets Year Prediction MSD and Electric Power Consumption it was good to look at the interquartile range which told us that $50\%$ of the predicted data was between the two quartiles. We think that this is a good measurement for the performance of an algorithm since mean and standard deviation were quite similar in most cases and for our question not really helpful. For AutoMPG looking only at the distribution was not sufficient, since the target dataset was simply to smallwas good to look at the interquartile range which told us that $50\%$ of the predicted data was between the two quartiles. We think that this is a good measurement for the performance of an algorithm since mean and standard deviation were quite similar in most cases and for our question not really helpful. For AutoMPG looking only at the distribution was not sufficient, since the target dataset was simply to small.
