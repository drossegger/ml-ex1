\section{Algorithms}
In this chapter, we explain the four main algorithms which are applied on all the datasets. As well as main algorithms, two essential additional ones are briefly described.

\subsection{Linear Ridge Regression}
Linear Regression fits a linear model with coefficients $w = (w_1, ..., w_p)$ to minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. Ridge Regression or Tikhonov regularization addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. In fact, the loss function is the linear least squares function and regularization is given by the $l2-norm$. The parameter $\alpha \geq 0$ is a complexity parameter that controls the amount of shrinkage. The complexity of Linear Ridge Regression is the same as Ordinary Lease Squares. If we consider $X$ as a matrix of size $(n, p)$, this method has a cost of $O(n p^2)$, assuming that $n \geq p$ ~\cite{alg:lrr}.

\subsection{$k$-nearest neighbors}
is a simple learning algorithm which can be used both for regression and classification. For regression it determines the result by simply taking the average of the values of the $k$-nearest neighbors. In most implementations the averages have a weight, thus the algorithm prefers choices nearer to the dataset. The result is strongly affected by the choice of $k$ and for datasets with many features the algorithm is known to have problems. The runtime of the algorithm is highly dependent on its implementations, the most naive implementation needs to compute distances between the target instance and all samples in the dataset and is therefore bound by $\mathcal{O}(dn^2)$ with $d$ being the dimension of the dataset (number of features) and $n$ the number of samples. Better implementations using specialized tree datastructures run in $\mathcal{O}(d\log n)$~\cite{alg:knn}.
\subsection{Support Vector Machine}

The term Support Vector Machine basically refers to a very specific class of
algorithms that use kernels. They were invented by Vladimir Vapnik and his
co-workers and first introduced in 1992 \cite{svmorg}. Another extension to this
algorithm was introduced in 1995 which was called soft margin version and it has
 application in many scenarios whereas the original one still could end up
 with better results in many other cases.\\
Support Vector Machine can be utilized not only in classification, but also in
regression and they are called Support Vector Regression (SVR). SVR still
contains all the main features that characterize maximum margin algorithm where
the capacity of the system can be controlled by parameters that do not depend on
the dimensionality of feature space.\\
Altought support vector machines are notably usefull, their cost in terms of
computation and memory increase rapidly with the number of training vectors. The
core of support vector machine is a quadratic programming problem (QP). The QP
solver \cite{alg:svm} which is the base of the tools used for regression in this
report scales between $O(n_{features} \times n_{samples}^2)$ and $O(n_{features} \times
n_{samples}^3)$. Overall, they seem useful but rathre expensive to build a model
in practice for large-scale data.
\subsection{Stochastic Gradient Descent}
In contrast to normal Regression algorithms Stochastic Gradient Regression does not compute the regression function by looking at all samples, instead it calculates the function for one sample and afterwards updates it with information gathered from other samples. It is highly dependent of the loss function, which is the underlying algorithm used to compute the function for one element (for instance ridge regression) and the penalty which is applied to the function based on the outcome of the loss function. The algorithm produces good results pretty fast usually. It's runtime is bound by $\mathcal{O}(knp)$~\cite{alg:sgd} where $k$ is the number of iterations and $p$ is the number of non-zero features. 
\subsection{Additional Algorithms}
\subsubsection{Polynomial Ridge Regression}
By altering Linear Ridge Regression algorithm and adding more attributes based on exponent value, we prepared Polynomial Ridge Regression.
\subsubsection{Neural Networks}
The inspiration for Neural Networks came from examination of central nervous systems. Neural Networks are typically organized in layers. Layers are made up of a number of interconnected nodes which contain an activation function. Patterns are presented to the network via the input layer, which communicates to one or more hidden layers where the actual processing is done via a system of weighted connections. The hidden layers then link to an output layer where the answer is output. The complexity regarding to Neural Network strongly depends on the structure of network and is out of the scope of the report.~\cite{alg:nn}