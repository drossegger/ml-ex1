\section{Electric power consumption dataset}
\label{db:sec:ds3}
\subsection{Description}
The third dataset was again grabbed from the same resource as two datasets before.
The dataset is called 'Individual household electric power consumption Data
Set\cite{ds:household}' and its data illustrate measurements of
one-minute-sampling electric power consumption in households over almost $4$
years. Originally, the dataset contains $2075259$ data instances gathered since
December $2006$ to November $2010$. In addition, each instance contains three
sub-metering values each corresponding to power consumption of some specific types
of home appliances and electrical equipments used in the house for which the
data relates. The dataset contains some missing values in the measurements.
Referring to the information attached to the data set, nearly $1.25\%$ of
instances contain missing values. Detailed description of data columns can be
found in Table~\ref{ypmsd:table:ds3attributes}.\\
However they are not used as they can be seen in the table. In the other words,
the data set used for prediction has some changes in the number of attributes
and their types and values.
This diversity comes from combining the last three attributes in order to acquire consumption by union of all devices
belonging to each specific group of metering and some mappings to new
environments.
Thus rather than three attributes for metering, one attribute is substituted, produced by summing all three. Also the
goal defined for prediction by regression in fact aims to give a predicted value
of metering consumption by all stated devices while having other parameters.
You can find more on data mappings in section \ref{ds3:Preprocessing}.








\subsection{Preprocessing\label{ds3:Preprocessing}}

In order to use data for regression, some steps are required to prepare data. These
steps include a mapping from the two first attributes, date and time, to numeric
values, if not they are useless in regression. The very first approach, and partially
the best so far, is to use Epoch\cite{ritchie1971unix} for both date and time.
However due to dependency of some regression algorithms intended to use for
regression on this data to distances between values of attribute instances, it
makes sense to either normalize time or use another type of mapping.
Consequently, using a linear function which maps time to its corresponding total
seconds, time is mapped to a new range of values. Nonetheless, Epoch still is
used to represent a date.\par
Next, we have to combine three last attributes and generate a new one based on
these three. Since there are some missing values in meterings and regarding the
fact that at most $1.25\%$ of data is missing, all instances which have a missing
value in any of these three attributes are eliminated. So far, the new data set
built on electric power consumption dataset is ready to be fed to regression
algorithms. \par
Nevertheless, another idea that bounces around is if normalisation of
data also affects the result of algorithms which is discussed later in this
article.\par
Besides, a portion of this data is cut to be used as test data. In the results
which follow, all experiments are carried out with randomly-chosen $10\%$ of data
cut out as test data.

\subsection{Linear Ridge Regression}

Following the principle of the previous two datasets, the same about linear ridge
regression is tried on current dataset meaning four different variations of
linear ridge regression obtained by replacing  the $\alpha$-value. Detailed
information and results are depicted in Table~\ref{ypmsd:table:ds3lrrresults}.
\\
As it is noticeable in the table, there is no touchable improvement earned while
playing with $\alpha$ in this dataset. Thus the same interpretation of gained
results in section~\ref{ds2:lrr} could also be made here. Nevertheless dealing
with such a dataset, also the runtime for making the model stays more or less the same for all
$\alpha$ values.



\subsection{$k$-nearest neighbors}

Again following the method the previous two
datasets, three different variations of $k$-nearest neighbors with
$k\in\{2, 5, 10\}$ are selected to be applied against the current dataset. Detailed
results are depicted in Table~\ref{ypmsd:table:ds3knnresults}.\\
According to these results, choosing $k$ as 2 has the lowest standard deviation
among all three. Meanwhile, choosing $k$ as 10 leads to a slighly better mean.
However, the difference is not noteworthy.


\subsection{Support Vector Machine}

As described in section~\ref{ds2:svm}, support vector machine suffers a time
difficulty in building a model for large datasets. As a result, this method is skipped in
this section. Hence, no result is shown regarding applying support vector
machine on current dataset.

\subsection{Stochastic Gradient Descent}

Applying stochastic gradient descent with two different loss functions, {\it
squared\_loss} and {\it huber}, each one with two variants $\epsilon \in
\{0.1, 1000\}$ results what is depicted in
Table~\ref{ypmsd:table:ds3sgdresults}. What is interesting is observing superior
outcome with smaller $\epsilon$ regardless of selecting loss function in terms
of standard deviation. Not only smaller $\epsilon$ leads to a better solution,
but also it seems that selecting {\it huber} as loss function helps to improve
the solution.
What's more, as it is noticable, {\it squared\_loss} needs relatively more time
for building its model.

\subsection{Normalisation}
It was mentioned before that there was an idea floating around to normalise
and scale Epoch numbers used instead of dates. Most impacts were
expected to happen to algorithms which rely on distances between instances such
as $k$-nearest neighbors. Surprisingly not only it didn't
improve any result, but also in most cases caused models to make more mistakes
and to provide worse results. Thus normalisation was discarded for this
dataset.

\subsection{Comparison}

As a conclusion to this section, a comparison among the best
configurations of each algorithm applied on the dataset is provided. As you can
see in Table~\ref{ypmsd:table:ds3comparison}, each algorithm has its own
benefits and selecting one as the best strictly depends on expectations.
While Linear ridge regression provides us with considerably lower model-building runtime
among all three, more precise results of $k$-nearest neighbors is undeniable.
















\begin{table}[p]
	\begin{center}
		\begin{tabular}{|p{4cm}|p{10cm}|}
			\hline	{\bf Attribute label}&{ \bf Description}\\
\hline date &   Date in format dd/mm/yyyy\\\hline
time &  	Time in format hh:mm:ss  \\\hline 
global\_active\_power & household global minute-averaged active power (in
kilowatt)  \\
\hline global\_reactive\_power & household global minute-averaged reactive power
(in kilowatt)  \\
\hline voltage & minute-averaged voltage (in volt)  \\
\hline global\_intensity & household global minute-averaged current intensity
(in ampere)  \\
\hline sub\_metering\_1 & energy sub-metering No. 1 (in watt-hour of active
energy). It corresponds to the kitchen, containing mainly a dishwasher, 
an oven and a microwave (hot plates are not electric but gas powered).  \\
\hline sub\_metering\_2 & energy sub-metering No. 2 (in watt-hour of active
energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.  \\
\hline sub\_metering\_3 & energy sub-metering No. 3 (in watt-hour of active 
energy). It corresponds to an electric water-heater and an air-conditioner.
\\\hline
\end{tabular}
\end{center}
\caption{Attrribute description of electric power consumption
dataset\label{ypmsd:table:ds3attributes}}

\end{table}





\begin{table}[p]
\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline \multirow{3}{*}{$Alpha (\alpha)$} &&& $1^{st}$ quartile
			&\\&Mean&Standard Deviation&Median& Runtime (s)\\&&& $3^{rd}$ quartile &\\
			\hline \multirow{3}{*}{$0.1$}&&&$-3.19193$&\\
			& $0.05528$ & $6.70155$  & $-0.07853$ & $0.252$\\
			&&&$1.97658$& \\
			\hline \multirow{3}{*}{$0.5$}&&&$-3.19192$ &\\
			& $0.05528$ & $6.70155$  & $-0.07853$ & $0.267$\\
			&&&	$1.97658$& \\
			\hline \multirow{3}{*}{$1$}&&&	$-3.19192$ &\\
			& $0.05528$ & $6.70155$  & $-0.07852$ & $0.271$\\
			&&&	$1.97658$& \\
			\hline \multirow{3}{*}{$10$}&&&	$-3.19191$ &\\
			& $0.05528$ & $6.70155$  & $-0.07846$ & $0.267$\\
			&&&	$1.97659$& \\
			\hline

		\end{tabular}
	\end{center}
\caption{Electric power consumption dataset - Linear Ridge
Regression\label{ypmsd:table:ds3lrrresults}}

\end{table}




\begin{table}[p]
\begin{center} 
\begin{tabular}{|c|c|c|c|c|}
			\hline \multirow{3}{*}{$k$} &&& $1^{st}$ quartile
			&\\&Mean&Standard Deviation&Median& Runtime (s)\\&&& $3^{rd}$ quartile &\\
			\hline \multirow{3}{*}{$2$}&&&$-0.50000$&\\
			& $0.02874$ & $3.88777$  & $0.00000$ & $5.975$\\
			&&&$0.50000$& \\
			\hline \multirow{3}{*}{$5$}&&&$-0.40000$ &\\
			& $0.02352$ & $3.89919$  & $0.00000$ & $5.795$\\
			&&&	$0.60000$& \\
			\hline \multirow{3}{*}{$10$}&&&	$-0.50000$ &\\
			& $0.01156$ & $4.06474$  & $0.00000$ & $5.782$\\
			&&&	$0.60000$& \\
			\hline

		\end{tabular}
	\end{center}
\caption{Electric power consumption dataset - $k-$Nearest Neighbors\label{ypmsd:table:ds3knnresults}}

\end{table}




\begin{table}[p]
\begin{center} 
\begin{tabular}{|c|c|c|c|c|c|}
			\hline \multirow{3}{*}{$Epsilon (\epsilon)$} &&&& $1^{st}$ quartile
			&\\&loss function&Mean&Standard Deviation&Median& Runtime (s)\\&&&& $3^{rd}$
			quartile &\\
			\hline \multirow{3}{*}{$0.1$}&&&&$-2.23407$&\\
			& huber & $0.80390$ & $6.81799$  & $0.02416$ & $4.388$\\
			&&&&$1.93741$& \\
			\hline \multirow{3}{*}{$0.1$}&&&&$-3.21330$ &\\
			& squared\_loss & $0.02817$ & $6.70329$  & $0.01763$ & $5.018$\\
			&&&&	$2.02225$& \\
			\hline \multirow{3}{*}{$1000$}&&&&	$-3.1139$ &\\
			& huber & $0.10550$ & $6.70759$  & $-0.1738$ & $4.411$\\
			&&&&	$1.8640$& \\
			\hline \multirow{3}{*}{$1000$}&&&&	$-3.0641$ &\\
			&squared\_loss & $0.15640$ & $6.71322$  & $-0.1773$ & $5.320$\\
			&&&&	$1.8581$& \\
			\hline

		\end{tabular}
	\end{center}

\caption{Electric power consumption dataset - Statistic Gradient Descent\label{ypmsd:table:ds3sgdresults}}

\end{table}



\begin{table}[p]
\begin{center} 
\begin{tabular}{|c|c|c|c|c|}
			\hline \multirow{3}{*}{Algorithm} &&& $1^{st}$ quartile
			&\\&Mean&Standard Deviation&Median& Runtime (s)\\&&& $3^{rd}$ quartile &\\
			\hline \multirow{3}{*}{Linear Ridge
Regression}&&&$-3.19193$&\\
			& $0.05528$ & $6.70155$  & $-0.07853$ & $\bf {0.252}$\\
			 ($\alpha = 0.1$)&&&$1.97658$& \\
			\hline \multirow{3}{*}{$k-$Nearest Neighbors}&&&$-0.50000$&\\
			& $\bf {0.02874}$ & $\bf {3.88777}$  & $0.00000$ & $5.975$\\
			 ($k = 2$)&&&$0.50000$& \\
			\hline \multirow{3}{*}{Statistic Gradient Descent }&&&$\bf {-2.23407}$&\\
			& $0.80390$ & $6.81799$  & $0.02416$ & $4.388$\\
			({\it huber}, $\epsilon = 0.1$ )&&&$\bf {1.93741}$& \\
			\hline

		\end{tabular}
	\end{center}
\caption{Electric power consumption dataset - A comparison over all applied
algorithms\label{ypmsd:table:ds3comparison}}

\end{table}
