Although there are hugh disparities between datasets in the concept, number of
attributes and number of instances, some general interesting fact can be pointed
out after this set of experiment.

Very first conclusion hitting mind is that $k-$Nearest Neighbors despite being
fairly simple in mechanism, results very good in all datasets, regardless of
their shape and size.

Support vector machines, theoretically must lead to sophisticated results but it
turned out that a dataset going bigger in size meaning larger in dimensionality
and higher in the number of instances, support vector machines will need a
considerable time to learn the structure behind the data.

Surprisingly, Neural Networks showed lack of ability to predict well for these
three kind of data. Nonetheless, it may also be caused by poor implementation of
this algorithm in SciKit or lack of knowledge by experimenters to efficiently
utilize this algorithm.

Decision Tree as well stands beside $k-$Nearest Neighbors as one of the
favorites by experimenters not only due to its relatively fine result, but also
being so fast in learning.


