Although there are huge disparities between the datasets in concept, number of
attributes and number of instances, some general interesting fact can be pointed
out after this set of experiments.

The first conclusion hitting mind is that $k-$Nearest Neighbors despite being
fairly simple in mechanism, has very good results in all datasets, regardless of
their shape and size.

Support vector machines, theoretically must lead to sophisticated results but it
turned out that a dataset going bigger in size, meaning larger in dimensionality
and higher in the number of instances, support vector machines will need a
considerable amount of time to learn the structure behind the data.

Surprisingly, Neural Networks showed lack of ability to predict well for these
three datasets. This might also be caused by poor implementation of
this algorithm in pybrain, but evidence of that could not be found.

Decision Tree as well stands beside $k-$Nearest Neighbors as one of the
favorites by experimenters not only due to its relatively fine result, but also
being fast in learning.

Interestingly, the results of Logistic Regression with the best parameter tuning and SVM seem very similar. While Logistic Regression has usually slightly worse results, a resemblance can be inferred to some extent.
